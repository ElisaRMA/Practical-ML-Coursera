---
title: "Machine Learning Project - Weight Lifting Exercise Dataset"
author: "ElisaRMA"
date: "7/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

This report was submitted as the final project for the [Practical Machine Learning Course] (https://www.coursera.org/learn/practical-machine-learning?specialization=jhu-data-science)

The goal of this project was to predict the manner in which subjects did the exercise. The datasets were obtained from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv and 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


# Loading datasets

The first step to start this project was loading the datasets. The datasets were already separated in training and testing sets. The function `read.csv` was used to load the data into R. 

```{r loading}

training <- read.csv("D:/Users/Elisa/Desktop/ML coursera/pml-training.csv")
testing <- read.csv("D:/Users/Elisa/Desktop/ML coursera/pml-testing.csv")

```


# Exploratory analysis of data and Data processing 

As the data was already separated, data leakage was prevented and the following steps of processing and transformations of data could be done. 

First it was observed that only 406 rows were complete, in other words, did not contain NAs, while 19216 rows had some NA value.

```{r completecases}
sum(complete.cases(training))
```

```{r NAcheck}
sum(!complete.cases(training))
```
So to better understand which columns had NAs on them the `is.na` function was applied into the `colSums` function. This would sum the NAs by column. Then, applying this function into `data.frame` function the result was tabulated and saved into another table, for easier visualization. 

```{r exploratory2}

NAs <- data.frame(colSums(is.na(training)))
NAs

```
As observed, the columns with 19216 NAs were the columns generated by calculations done with other features, therefore, these were highly correlated with 'raw' features. The authors calculated mean, variance,
standard deviation, max, min, amplitude, kurtosis and skewness of the data.

With this information, then, these columns were deleted from the training dataset, so both NAs and correlated features were removed.  

This was done using `grep`function to the column names of the training set and subsetting all but the columns specified in grep. 

```{r knnimpute}

training_proc <- training[, -grep(c("^avg|^var|^kurtosis|^max|^min|^amplitude|^skewness|^stddev"),
                                  colnames(training))]

```

A second check up was performed to check the presence of NAs and, as observed, no NAs were left. 

```{r NA 2nd check}

sum(!complete.cases(training_proc))

```
Once the NAs were removed, relationships between other variables should be accounted for and deleted to avoid adding unnecessary variables to the ML model. For this, only numeric variables were selected and a correlation matrix was created. Such matrix were inputted into the `findCorrelation` function of `caret` package, to detect which features had correlations with each other larger than 0.9.  

```{r}

library(dplyr)
library(caret)   

#to make the correlation matrix only numeric variables could be used
training_num <- select_if(training_proc, is.numeric)

#cor matrix
cormatrix <- cor(training_num)

#detecting variables highly correlated 
cor<- cormatrix> 0.90

cor_features <- findCorrelation(cormatrix, cutoff = 0.9, verbose = FALSE, names = FALSE, 
                                exact = ncol(cormatrix) < 100)

#removing these features
training_final <- training_num[,-c(cor_features)]

#remaking the training data
training_final$classe <- training_proc$classe
training_final <- training_final[,-1]
```


# Model Selection

With the final training dataset, the model selection was performed. For this cross, validation was performed. First, the dataset was divided into 5 pieces(5-fold) and training and validation were performed with different combinations of this divided dataset, 5 times. The random seed number was set before each step to ensure that each algorithm got the same data partitions and repeats. This made the comparison trustworthy.
Two algoriths were testes: Decision Tree and Random Forest as both are recommended for classification problems. 

```{r }
#kfold and repetition
set.seed(123)
control <- trainControl(method="repeatedcv", number=5, repeats=5)

```

```{r}
#Tree
set.seed(123)
model1 <- train(classe~., data=training_final, method="rpart", trControl=control)
        
```

```{r}
#Random Forest
set.seed(123)
model2 <- train(classe~., data=training_final, method="rf", trControl=control)

```

The results of the cross validation were then summarized for analysis.
```{r}
results <- resamples(list(Tree=model1, RF=model2))
# summarize the distributions
summary(results)
```

As observed, the Random Forest model had an higher accuracy and kappa, so it was chosen as the model. 

# Applying models

The chosen model, random forest, was used to predict the `classes` variable of the `testing` dataset. 

```{r, prediction }

predict(model2, testing)


```

# Out of sample error

To verify the out of sample error it is possible to apply the predict function using the chosen model and the ´type´ prob, to get the probability of each class, for each subject. For example, with this, it is possible to estimate the out of sample error for the first subject, that was classified with the B 'classe' but there was a 0.004 change to be at 'classe' A, 0.028 chance to be at 'classe' C, 0.002 chance to be at D and 0.006 to be at 'classe' E.

```{r}

predict(model2, testing, type = 'prob')

```

# Conclusion

For this dataset and analysis, the best chosen model was Random Forest with a 99% accuracy and kappa, with a small out of sample error 
